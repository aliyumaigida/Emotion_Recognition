{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPftj/pKt7fEpzj1t3JOqJm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Mounting the drive and unziping the Data Folder"],"metadata":{"id":"fiXLy3Uqdnhe"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ojo5oPGV-AVa","executionInfo":{"status":"ok","timestamp":1760702489866,"user_tz":-60,"elapsed":21680,"user":{"displayName":"Maigida Aliyu","userId":"05143264407796964054"}},"outputId":"c5b9625a-b93b-47af-be39-b7e14bcd1907"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","processed_data\n"]}],"source":["from google.colab import drive\n","import zipfile\n","import os\n","\n","drive.mount('/content/drive')\n","\n","zip_path = '/content/drive/MyDrive/processed_data.zip'  # your ZIP file\n","extract_path = '/content/processed_data'\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_path)\n","\n","# Check contents\n","!ls /content/processed_data\n"]},{"cell_type":"markdown","source":["## Moving all emotion folders to root"],"metadata":{"id":"UeI0k0qeeCiu"}},{"cell_type":"code","source":["import shutil\n","import os\n","\n","src_inner = '/content/processed_data/processed_data'\n","dst_root = '/content/processed_data'\n","\n","# Move all emotion folders to root\n","for item in os.listdir(src_inner):\n","    shutil.move(os.path.join(src_inner, item), dst_root)\n","\n","# Remove now-empty folder\n","os.rmdir(src_inner)\n","\n","# Verify\n","!ls /content/processed_data\n"],"metadata":{"id":"h2qtGe0cOsQ1","executionInfo":{"status":"ok","timestamp":1760702921097,"user_tz":-60,"elapsed":157,"user":{"displayName":"Maigida Aliyu","userId":"05143264407796964054"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb9c5209-e787-44a4-8c07-0bfe461afe4d"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["angry  disgust\tfear  happy  neutral  sad  surprise  train  val\n"]}]},{"cell_type":"markdown","source":["## Splitting the Data into train and val"],"metadata":{"id":"ojxN4-6QeTho"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from glob import glob\n","import shutil\n","import os\n","\n","data_dir = '/content/processed_data'\n","train_dir = os.path.join(data_dir, 'train')\n","val_dir   = os.path.join(data_dir, 'val')\n","\n","# Create train/val folders if they don't exist\n","os.makedirs(train_dir, exist_ok=True)\n","os.makedirs(val_dir, exist_ok=True)\n","\n","# Loop through emotion folders\n","for emotion in ['angry','disgust','fear','happy','neutral','sad','surprise']:\n","    emotion_path = os.path.join(data_dir, emotion)\n","    images = glob(os.path.join(emotion_path, '*.jpg'))\n","    if not images:\n","        continue\n","\n","    train_imgs, val_imgs = train_test_split(images, test_size=0.2, random_state=42)\n","\n","    os.makedirs(os.path.join(train_dir, emotion), exist_ok=True)\n","    os.makedirs(os.path.join(val_dir, emotion), exist_ok=True)\n","\n","    # Copy images\n","    for img in train_imgs:\n","        shutil.copy(img, os.path.join(train_dir, emotion))\n","    for img in val_imgs:\n","        shutil.copy(img, os.path.join(val_dir, emotion))\n","\n","print(\"✅ Dataset properly split into 'train' and 'val' folders!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y4QmCYkE-GSS","executionInfo":{"status":"ok","timestamp":1760702947535,"user_tz":-60,"elapsed":8530,"user":{"displayName":"Maigida Aliyu","userId":"05143264407796964054"}},"outputId":"aa164ab9-a946-4299-d3aa-f9fb3b36c20d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Dataset properly split into 'train' and 'val' folders!\n"]}]},{"cell_type":"markdown","source":["## To be Show of my Directory Where my Data is"],"metadata":{"id":"TJSs9KMeemT7"}},{"cell_type":"code","source":["!ls /content/processed_data/train\n","!ls /content/processed_data/val\n"],"metadata":{"id":"mHu7URdGEbXs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760702966157,"user_tz":-60,"elapsed":219,"user":{"displayName":"Maigida Aliyu","userId":"05143264407796964054"}},"outputId":"ecf9e3bf-153d-47a9-95b0-2ba35bcd79db"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["angry  disgust\tfear  happy  neutral  sad  surprise\n","angry  disgust\tfear  happy  neutral  sad  surprise\n"]}]},{"cell_type":"markdown","source":["## Calculate class weights based on the frequency of classes"],"metadata":{"id":"DCrhoiO9GWpw"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Calculate class weights based on the frequency of classes\n","class_counts = [len(os.listdir(f'/content/processed_data/train/{cls}')) for cls in train_dataset.classes]\n","class_weights = [sum(class_counts) / count for count in class_counts]\n","class_weights = torch.tensor(class_weights).float().cuda()\n","\n","# Use class weights in your loss function\n","criterion = nn.CrossEntropyLoss(weight=class_weights)\n"],"metadata":{"id":"B4gj44Br_MfN","executionInfo":{"status":"ok","timestamp":1760702974414,"user_tz":-60,"elapsed":13,"user":{"displayName":"Maigida Aliyu","userId":"05143264407796964054"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["## Data Preprocessing"],"metadata":{"id":"0cdDkQqifzu2"}},{"cell_type":"code","source":["from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize((224,224)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomRotation(10),  # Random rotation for better generalization\n","        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Color jitter\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.0911, -0.0043, -0.0403], [0.3940, 0.3806, 0.3741])  # Your calculated mean and std\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize((224,224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.0911, -0.0043, -0.0403], [0.3940, 0.3806, 0.3741])  # Same normalization for validation\n","    ])\n","}\n","\n","\n","train_dataset = datasets.ImageFolder('/content/processed_data/train', transform=data_transforms['train'])\n","val_dataset   = datasets.ImageFolder('/content/processed_data/val', transform=data_transforms['val'])\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader   = DataLoader(val_dataset, batch_size=32)\n","\n","print(\"✅ Dataset loaded!\")\n","print(\"Training images:\", len(train_dataset))\n","print(\"Validation images:\", len(val_dataset))\n","print(\"Classes:\", train_dataset.classes)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"woTe4i6RFq3i","executionInfo":{"status":"ok","timestamp":1760702992964,"user_tz":-60,"elapsed":9082,"user":{"displayName":"Maigida Aliyu","userId":"05143264407796964054"}},"outputId":"b042d90e-dda4-4ecd-bda3-19d6a632b80e"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Dataset loaded!\n","Training images: 47772\n","Validation images: 17907\n","Classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"]}]},{"cell_type":"markdown","source":["## Training the model"],"metadata":{"id":"0tuSlnoPfZrN"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from facenet_pytorch import InceptionResnetV1\n","from tqdm import tqdm\n","\n","# Check device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# Load pretrained model\n","model = InceptionResnetV1(pretrained='vggface2', classify=True, num_classes=7).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Training loop\n","epochs = 10\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item() * images.size(0)\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","\n","    # Validation\n","    model.eval()\n","    correct, total = 0, 0\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, preds = torch.max(outputs, 1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","\n","    val_acc = correct / total\n","    print(f\"Epoch {epoch+1}/{epochs} — Loss: {epoch_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","print(\"✅ Training completed!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdO3kmJAPSQy","outputId":"04fb7a96-de22-43d0-85df-6de2db93f5bc","executionInfo":{"status":"ok","timestamp":1760694753048,"user_tz":-60,"elapsed":179403,"user":{"displayName":"Maigida Aliyu","userId":"05143264407796964054"}}},"execution_count":7,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 1/10: 100%|██████████| 1495/1495 [11:22<00:00,  2.19it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 1/10 — Loss: 0.9863, Val Acc: 0.6732\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 2/10: 100%|██████████| 1495/1495 [11:21<00:00,  2.20it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 2/10 — Loss: 0.8163, Val Acc: 0.7315\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 3/10: 100%|██████████| 1495/1495 [11:24<00:00,  2.18it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 3/10 — Loss: 0.7590, Val Acc: 0.7541\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 4/10: 100%|██████████| 1495/1495 [11:19<00:00,  2.20it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 4/10 — Loss: 0.6829, Val Acc: 0.7728\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 5/10: 100%|██████████| 1495/1495 [11:19<00:00,  2.20it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 5/10 — Loss: 0.6411, Val Acc: 0.8005\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 6/10: 100%|██████████| 1495/1495 [11:20<00:00,  2.20it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 6/10 — Loss: 0.5801, Val Acc: 0.8327\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 7/10: 100%|██████████| 1495/1495 [11:18<00:00,  2.20it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 7/10 — Loss: 0.5345, Val Acc: 0.8552\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 8/10: 100%|██████████| 1495/1495 [11:15<00:00,  2.21it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 8/10 — Loss: 0.4919, Val Acc: 0.8661\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Epoch 9/10: 100%|██████████| 1495/1495 [11:16<00:00,  2.21it/s]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Epoch 9/10 — Loss: 0.4405, Val Acc: 0.8703\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10: 100%|██████████| 1495/1495 [11:17<00:00,  2.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10/10 — Loss: 0.4035, Val Acc: 0.9003\n","✅ Training completed!\n"]}]},{"cell_type":"markdown","source":["## Saving the model"],"metadata":{"id":"UXin1oxF0U4o"}},{"cell_type":"code","source":["import torch\n","\n","# Make sure 'model' is defined in this session (your trained InceptionResnetV1)\n","save_path = '/content/drive/MyDrive/emotion_model2.pth'  # You can change folder/name if you want\n","torch.save(model.state_dict(), save_path)\n","\n","print(f\"✅ Model saved successfully at {save_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OsVhDGA1249-","executionInfo":{"status":"ok","timestamp":1760694772638,"user_tz":-60,"elapsed":325,"user":{"displayName":"Maigida Aliyu","userId":"05143264407796964054"}},"outputId":"d531ad37-9277-4f62-84ac-279e1b80f4d2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Model saved successfully at /content/drive/MyDrive/emotion_model2.pth\n"]}]},{"cell_type":"markdown","source":["## Reloading the model again from the Directory"],"metadata":{"id":"YqLCIvIf_iA7"}},{"cell_type":"code","source":["import torch\n","from facenet_pytorch import InceptionResnetV1\n","\n","# Define the model architecture\n","model = InceptionResnetV1(pretrained='vggface2', classify=True, num_classes=7)\n","\n","# Load the trained weights\n","model.load_state_dict(torch.load('/content/drive/MyDrive/emotion_model2.pth'))\n","model.eval()  # Set to evaluation mode\n","\n","# Move to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","print(\"✅ Model loaded successfully!\")\n"],"metadata":{"id":"O5_MNnJvPpMx","executionInfo":{"status":"ok","timestamp":1760694780844,"user_tz":-60,"elapsed":730,"user":{"displayName":"Maigida Aliyu","userId":"05143264407796964054"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6ef5bd64-1fd1-4ca3-ed0b-df8cad0b9e7a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Model loaded successfully!\n"]}]},{"cell_type":"markdown","source":["## Testing the model on the Validation Data"],"metadata":{"id":"QoLLPJKxDARc"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","from facenet_pytorch import InceptionResnetV1\n","from tqdm import tqdm\n","\n","# Device setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)\n","\n","# Paths (update if needed)\n","val_dir = \"/content/processed_data/val\"\n","model_path = \"/content/drive/MyDrive/emotion_model2.pth\"\n","\n","# Define same transforms used during training\n","data_transforms = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.0911, -0.0043, -0.0403], [0.3940, 0.3806, 0.3741])\n","])\n","\n","# Load validation dataset\n","val_dataset = datasets.ImageFolder(val_dir, transform=data_transforms)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","# Load model\n","model = InceptionResnetV1(pretrained='vggface2', classify=True, num_classes=7)\n","model.load_state_dict(torch.load(model_path, map_location=device))\n","model.eval()\n","model = model.to(device)\n","\n","# Validation loop\n","correct = 0\n","total = 0\n","running_loss = 0.0\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","with torch.no_grad():\n","    for images, labels in tqdm(val_loader, desc=\"Validating\"):\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        running_loss += loss.item() * images.size(0)\n","\n","        _, preds = torch.max(outputs, 1)\n","        correct += (preds == labels).sum().item()\n","        total += labels.size(0)\n","\n","val_acc = correct / total\n","val_loss = running_loss / len(val_loader.dataset)\n","\n","print(f\"\\n✅ Validation completed!\")\n","print(f\"Validation Accuracy: {val_acc:.4f}\")\n","print(f\"Validation Loss: {val_loss:.4f}\")\n"],"metadata":{"id":"zHp8yG1gBFor","executionInfo":{"status":"ok","timestamp":1760695127956,"user_tz":-60,"elapsed":77203,"user":{"displayName":"Maigida Aliyu","userId":"05143264407796964054"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fe247ae7-5e07-455b-9cf3-6aaff4bba8d5"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Validating: 100%|██████████| 561/561 [01:16<00:00,  7.37it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","✅ Validation completed!\n","Validation Accuracy: 0.9003\n","Validation Loss: 0.2925\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","source":["## USING WEBCAM TO COLLECT INPUT IT IS NOT WORKING ON COLAB ONLY ON YOUR LOCAL MACHINE"],"metadata":{"id":"9rzf-nBV-8C9"}},{"cell_type":"code","source":["import cv2\n","import torch\n","from facenet_pytorch import InceptionResnetV1\n","from torchvision import transforms\n","from PIL import Image\n","\n","# Load model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = InceptionResnetV1(pretrained='vggface2', classify=True, num_classes=7)\n","model.load_state_dict(torch.load('/content/drive/MyDrive/emotion_model2.pth'))  # path to your saved model\n","model.eval()\n","model = model.to(device)\n","\n","# Define transform\n","transform = transforms.Compose([\n","    transforms.Resize((224,224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.0911, -0.0043, -0.0403], [0.3940, 0.3806, 0.3741])\n","])\n","\n","# Class labels\n","classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n","\n","# Start webcam\n","cap = cv2.VideoCapture(0)\n","\n","while True:\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    # Convert frame to PIL image\n","    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n","    img_tensor = transform(img).unsqueeze(0).to(device)\n","\n","    # Predict emotion\n","    with torch.no_grad():\n","        outputs = model(img_tensor)\n","        _, pred = torch.max(outputs, 1)\n","        emotion = classes[pred.item()]\n","\n","    # Display the result\n","    cv2.putText(frame, f\"Emotion: {emotion}\", (10,30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n","    cv2.imshow(\"Webcam Emotion Recognition\", frame)\n","\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break\n","\n","cap.release()\n","cv2.destroyAllWindows()\n"],"metadata":{"id":"1PaJozW03Oob","executionInfo":{"status":"ok","timestamp":1760695156443,"user_tz":-60,"elapsed":584,"user":{"displayName":"Maigida Aliyu","userId":"05143264407796964054"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"uEwCR0wwEeHF"},"execution_count":null,"outputs":[]}]}